{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74bdd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "133d6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'archive/Dataset/Train'\n",
    "validation_dir = 'archive/Dataset/Validation'\n",
    "test_dir = 'archive/Dataset/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7bbe69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import os\n",
    "train_dir = 'archive/Dataset/Train'\n",
    "folder_name = 'Fake'\n",
    "image_name = 'fake_1.jpg'\n",
    "folder_path = os.path.join(train_dir, folder_name)\n",
    "image_path = os.path.join(folder_path, image_name)\n",
    "if os.path.exists(folder_path):\n",
    "    if os.path.exists(image_path):\n",
    "        image = Image.open(image_path)\n",
    "        image.show()\n",
    "    else:\n",
    "        print(f\"The image {image_name} does not exist in the folder {folder_name}.\")\n",
    "else:\n",
    "    print(f\"The folder {folder_name} does not exist in the train directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee097e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel(img, x1, y1, x, y):\n",
    "    new_value = 0\n",
    "    try:\n",
    "        if img[x1][y1] >= img[x][y]:\n",
    "            new_value = 1\n",
    "    except IndexError:\n",
    "        pass\n",
    "    return new_value\n",
    "\n",
    "def cs_lbp_calculated_pixel(img, x, y):\n",
    "    val_ar = []\n",
    "    \n",
    "    val_ar.append(get_pixel(img, x, y+1, x, y-1))\n",
    "    val_ar.append(get_pixel(img, x+1, y+1, x-1, y - 1))\n",
    "    val_ar.append(get_pixel(img, x+1, y, x-1, y))\n",
    "    val_ar.append(get_pixel(img, x+1, y-1, x - 1, y + 1))\n",
    "\n",
    "    power_val = [1, 2, 4, 8]\n",
    "    val = 0\n",
    "    for i in range(len(val_ar)):\n",
    "        val += val_ar[i] * power_val[i]\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091b99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cs_lbp_clahe(image, output_directory, label, clip_limit=2.0, grid_size=(8, 8)):\n",
    "    img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img_clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size).apply(img_gray)\n",
    "    height, width = img_clahe.shape\n",
    "    img_cs_lbp = np.zeros((height, width), np.uint16)\n",
    "\n",
    "    for i in range(0, height):\n",
    "        for j in range(0, width):\n",
    "            img_cs_lbp[i, j] = cs_lbp_calculated_pixel(img_clahe, i, j)\n",
    "\n",
    "    # Save the CS-LBP image\n",
    "    cv2.imwrite(os.path.join(output_directory, label), img_cs_lbp)\n",
    "    \n",
    "    #plt.imshow(img_cs_lbp, cmap=\"gray\")\n",
    "    #plt.title(\"CS-LBP Image\")\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf75ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS-LBP transformation with CLAHE completed for all images in the train, validation, and test datasets.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define CLAHE parameters\n",
    "clip_limit = 2.0\n",
    "grid_size = (8, 8)\n",
    "\n",
    "# Function to apply CS-LBP and CLAHE transformations\n",
    "def apply_cs_lbp_clahe(img, output_dir, filename):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n",
    "    clahe_img = clahe.apply(gray)\n",
    "    \n",
    "    # Apply CS-LBP\n",
    "    # Assuming CS-LBP logic is implemented elsewhere\n",
    "    \n",
    "    # Save the processed image\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    cv2.imwrite(output_path, clahe_img)\n",
    "    \n",
    "# Function to extract face ROI from an image\n",
    "def extract_face(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Assuming face ROI extraction logic is implemented elsewhere\n",
    "    \n",
    "    return img # Placeholder return for demonstration\n",
    "\n",
    "# Define the function to process each dataset\n",
    "def process_dataset(real_dir, fake_dir, output_parent_dir):\n",
    "    for label, directory in zip(['Real', 'Fake'], [real_dir, fake_dir]):\n",
    "        output_dir = os.path.join(output_parent_dir, label)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Process each image in the directory\n",
    "        for image_file in os.listdir(directory):\n",
    "            image_path = os.path.join(directory, image_file)\n",
    "            face_roi = extract_face(image_path)\n",
    "            if face_roi is not None:\n",
    "                apply_cs_lbp_clahe(face_roi, output_dir, image_file)\n",
    "\n",
    "# Set the paths for each dataset\n",
    "train_real_dir = 'archive/Dataset/Train/Real'\n",
    "train_fake_dir = 'archive/Dataset/Train/Fake'\n",
    "test_real_dir = 'archive/Dataset/Test/Real'\n",
    "test_fake_dir = 'archive/Dataset/Test/Fake'\n",
    "validation_real_dir = 'archive/Dataset/Test/Fake'  # Should this be 'archive (2)/Dataset/Validation/Real'?\n",
    "validation_fake_dir = 'archive/Dataset/Validation/Fake'\n",
    "output_dir = 'archive/Dataset/output'\n",
    "\n",
    "\n",
    "# Process each dataset\n",
    "process_dataset(train_real_dir, train_fake_dir, os.path.join(output_dir, 'training'))\n",
    "process_dataset(test_real_dir, test_fake_dir, os.path.join(output_dir, 'testing'))\n",
    "process_dataset(validation_real_dir, validation_fake_dir, os.path.join(output_dir, 'validation'))\n",
    "\n",
    "print(\"CS-LBP transformation with CLAHE completed for all images in the train, validation, and test datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5559a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'archive/Dataset/output/training'\n",
    "validation_dir = 'archive/Dataset/output/training'\n",
    "test_dir = 'archive/Dataset/output/training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4de9ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5b65867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140002 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',  # Assuming a binary classification task (fake vs real)\n",
    "    classes=['Fake','Real']  # Specify the class names\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0d2f422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140002 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    classes=['Fake', 'Real']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c645f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140002 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    classes=['Fake', 'Real']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e88afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140002 images belonging to 2 classes.\n",
      "Found 39428 images belonging to 2 classes.\n",
      "Found 10905 images belonging to 2 classes.\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1486s\u001b[0m 338ms/step - accuracy: 0.7594 - loss: 0.4830 - val_accuracy: 0.7867 - val_loss: 0.4500\n",
      "Epoch 2/7\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1418s\u001b[0m 324ms/step - accuracy: 0.7961 - loss: 0.4274 - val_accuracy: 0.7833 - val_loss: 0.4557\n",
      "Epoch 3/7\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1423s\u001b[0m 325ms/step - accuracy: 0.7995 - loss: 0.4220 - val_accuracy: 0.7906 - val_loss: 0.4415\n",
      "Epoch 4/7\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1426s\u001b[0m 326ms/step - accuracy: 0.8002 - loss: 0.4226 - val_accuracy: 0.7734 - val_loss: 0.4749\n",
      "Epoch 5/7\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1426s\u001b[0m 326ms/step - accuracy: 0.8009 - loss: 0.4195 - val_accuracy: 0.7905 - val_loss: 0.4425\n",
      "Epoch 6/7\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1439s\u001b[0m 329ms/step - accuracy: 0.8027 - loss: 0.4177 - val_accuracy: 0.7816 - val_loss: 0.4642\n",
      "Epoch 7/7\n",
      "\u001b[1m4376/4376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1449s\u001b[0m 331ms/step - accuracy: 0.8011 - loss: 0.4188 - val_accuracy: 0.7947 - val_loss: 0.4355\n",
      "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 276ms/step - accuracy: 0.6969 - loss: 0.6415\n",
      "Test accuracy: 0.6953691244125366\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define directories for training, validation, and test data\n",
    "train_dir = 'archive/Dataset/Train'\n",
    "validation_dir = 'archive/Dataset/Validation'\n",
    "test_dir = 'archive/Dataset/Test'\n",
    "\n",
    "# Define image size and batch size\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Create ImageDataGenerator instances for training, validation, and test data\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input)\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='binary')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input)\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size=img_size, batch_size=batch_size, class_mode='binary')\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='binary')\n",
    "\n",
    "# Load MobileNetV2 model pre-trained on ImageNet dataset\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "# Freeze the pre-trained layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom classification head\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output_layer2 = tf.keras.layers.Dense(1, activation='sigmoid')(global_average_layer)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=output_layer2)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=7, validation_data=validation_generator)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47248b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m341/341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 255ms/step - accuracy: 0.6960 - loss: 0.6355\n",
      "Test accuracy: 0.6953691244125366\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a936446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1233/1233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 256ms/step - accuracy: 0.7936 - loss: 0.4392\n",
      "Test accuracy: 0.7947397828102112\n"
     ]
    }
   ],
   "source": [
    "validation_loss, validation_acc = model.evaluate(validation_generator)\n",
    "print('Test accuracy:', validation_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "518ddaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('my_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60382d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e49082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
